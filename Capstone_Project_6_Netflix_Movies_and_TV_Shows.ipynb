{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "yiiVWRdJDDil",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikamPratiksha0506/ML/blob/main/Capstone_Project_6_Netflix_Movies_and_TV_Shows.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Netflix Movies and TV Shows Clustering\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Netflix Movies and TV Shows dataset comprises 7,787 entries, offering detailed information about the platform's content. Key features include the type of content (Movie or TV Show), title, director, cast, country of origin, date added to Netflix, release year, rating (e.g., TV-MA, PG-13), duration (minutes or seasons), genres listed in (listed_in), and a short description. While most entries are complete, some columns like director and cast contain missing values, highlighting potential data-cleaning needs. This dataset provides rich categorical and textual data, ideal for clustering content into meaningful groups, such as by genre, country, or audience rating. By analyzing these clusters, insights into audience preferences, content diversity, and trends can be uncovered, making it a robust foundation for exploring Netflix's vast content library."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Netflix's extensive library of movies and TV shows spans multiple genres, countries, and audience categories. However, understanding patterns in this vast collection to improve user experience, personalize recommendations, and identify content gaps remains a challenge. The goal of this project is to leverage clustering techniques to group similar movies and TV shows based on their features such as genre, country, release year, duration, and audience rating. These clusters will help uncover insights into content trends, enhance recommendation systems, and assist in strategic content acquisition and production to meet diverse viewer preferences."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Clustering and preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#libraries used to process textual data\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the file path\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NrQWg4LfMZ7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "import os\n",
        "\n",
        "file_path = '/content/drive/My Drive/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv'\n",
        "\n",
        "print(os.path.exists(file_path))    # to know if the file exists at the specified path\n",
        "\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "rows, columns =data.shape\n",
        "print(f\"The dataset has {rows} rows and {columns} columns.\")    #Count the number of rows and columns\n",
        "\n",
        "\n",
        "print(data.index)\n",
        "print('\\n')\n",
        "print(data.columns)"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "\n",
        "duplicate_count = data.duplicated().sum()\n",
        "\n",
        "# Print the result\n",
        "print(f\"The dataset has {duplicate_count} duplicate rows.\")\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Sort missing values in descending order\n",
        "missing_val = data.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Display missing values\n",
        "print(missing_val)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))    # visualization of missing values heatmap\n",
        "sns.heatmap(data.isnull(), cbar=False, cmap='viridis')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains 7,787 entries and 12 columns, providing details about Netflix movies and TV shows. Key columns include type (Movie or TV Show), title, director, cast, country, release_year, rating, duration, and genres (listed_in). Missing values are present in columns like director, cast, country, and rating. Most data is categorical, requiring transformation for clustering, and numeric fields like release_year and duration will need preprocessing. The project involves cleaning the data, engineering features, and applying clustering techniques like K-Means or DBSCAN to identify patterns in content types or genres."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "dataset_columns= data.columns\n",
        "dataset_columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. show_id: Unique identifier for each show or movie.\n",
        "2. type: Type of content (e.g., TV Show, Movie).\n",
        "3. title: Title of the show or movie.\n",
        "4. director: Director(s) of the show or movie.\n",
        "5. cast: Main cast members of the show or movie.\n",
        "6. country: Country of origin.\n",
        "7. date_added: Date the show or movie was added to the platform.\n",
        "8. release_year: Year the show or movie was released.\n",
        "9. rating: Rating of the show or movie (e.g., TV-MA, R, PG-13).\n",
        "10. duration: Duration of the show or movie (e.g., number of seasons or       minutes).\n",
        "11. listed_in: Genres the show or movie is categorized under.\n",
        "12. description: A brief description of the show or movie plot."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for element in data.columns.tolist():\n",
        "     print(\"no of Unique values in\",element,\"is\",data[element].nunique())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "missing_val[:10]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Values: Filled missing values in director, cast, and country with \"Unknown\" and handled missing dates by converting date_added to datetime."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 1- Bar Chart"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "data['type'].value_counts().plot(kind='bar', color=['skyblue', 'lightcoral'], figsize=(8, 5))\n",
        "plt.title('Distribution of Content Types on Netflix')\n",
        "plt.xlabel('Content Type')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose a bar chart because it effectively compares the counts of Movies and TV Shows, making it easy to see which type dominates"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart reveals the distribution of Netflix content, showing whether Movies or TV Shows dominate the platform."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The insights can help optimize Movies and Tv Show, leading to increased Movies and profitability."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 2 - Pie Chart"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Count the occurrences of each country in the 'country' column\n",
        "country_count = data['country'].value_counts()\n",
        "\n",
        "# Select top 10 countries for the chart\n",
        "top_countries = country_count.head(10)\n",
        "\n",
        "# Plot the pie chart\n",
        "plt.pie(top_countries, labels=top_countries.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)\n",
        "plt.title('Top 10 Countries by Netflix Content')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pie chart is ideal for showing proportions, making it easy to visualize the distribution of Netflix content across different countries. It helps highlight the countries with the most content in a clear and intuitive way."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pie chart identifies which countries produce the most content for Netflix. A heavy presence of content from specific countries (like the U.S.) may reflect global market focus or content availability in those regions."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insights into which countries contribute most to the Netflix catalog allow Netflix to invest more in regions with strong growth potential, like expanding content in international markets to cater to a global audience."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 3 - Line Chart"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "release_year_count = data['release_year'].value_counts().sort_index()\n",
        "\n",
        "release_year_count.plot(kind='line', color='purple', marker='o')\n",
        "plt.title('Content Release Year Trend')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line chart effectively illustrates trends over time, helping to visualize changes in content release patterns year by year."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The line chart reveals how the volume of Netflix content released has changed over the years. A sharp increase in releases may indicate Netflix's growing catalog or expansion into more content types and regions."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rising trend in the number of releases indicates that Netflix is expanding its catalog and diversifying its offerings, which can help sustain subscriber growth and attract new users looking for variety in content."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 4 - Histogram"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "data['duration_numeric'] = data['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in str(x) else int(x.split()[0]))\n",
        "\n",
        "plt.hist(data['duration_numeric'], bins=30, color='lightcoral', edgecolor='black')\n",
        "plt.title('Content Duration Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram is chosen to show the distribution of content durations (seasons for TV shows and minutes for movies). It helps visualize how content is spread across different duration ranges, making it easy to see whether Netflix leans more towards shorter or longer content."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram reveals the most common content durations. If thereâ€™s a peak around shorter durations (e.g., 90 minutes), it suggests Netflix offers more movies or short TV shows. If there's a peak in longer durations, it indicates a greater focus on TV shows or longer movies."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the histogram shows a balanced distribution of durations, Netflix can continue offering a mix of short and long content, appealing to a wide audience. It helps in planning content strategies."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 5 - Scatter Plot"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Plot scatter plot: Duration vs. Release Year\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "data['duration_numeric'] = data['duration'].apply(lambda x: int(x.split()[0]) if 'Season' in str(x) else int(x.split()[0]))\n",
        "\n",
        "plt.scatter(data['release_year'], data['duration_numeric'], alpha=0.5, color='blue')\n",
        "plt.title('Duration vs. Release Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot is selected to show the relationship between the release year and content duration. This helps identify trends, such as whether Netflix is releasing more short content in recent years or if longer content is becoming more prevalent."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plot shows how content duration has evolved over the years. If more recent years show shorter durations, it might indicate a shift toward quick-viewing content. Alternatively, a trend towards longer durations over time could suggest Netflix's increasing focus on in-depth series or longer films."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If recent content leans toward shorter durations, Netflix can capitalize on the growing demand for quick entertainment (e.g., younger audiences or users with limited time)."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Ensure numerical-only columns for correlation computation\n",
        "numerical_data = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Compute the correlation matrix\n",
        "correlation_matrix = numerical_data.corr()\n",
        "\n",
        "# Set the size of the heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create the heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Add title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation heatmap is ideal for analyzing relationships between numerical variables, allowing quick identification of strong, weak, or negative correlations."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables with strong positive/negative correlations (e.g., sales vs. profit).\n",
        "Redundant features (high correlation suggests one may be removed).\n",
        "Patterns that can guide further analysis or decisions."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "# Create the pair plot\n",
        "sns.pairplot(data, diag_kind='kde', corner=True)  #Automatically plots scatter plots\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pair plot is chosen to visualize relationships between multiple numerical variables simultaneously, making it ideal for exploratory data analysis."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifies correlations or trends (e.g., one variable increasing with another).\n",
        "Highlights clusters or groups in the data.\n",
        "Reveals outliers or non-linear relationships.\n",
        "Shows variable distributions for individual features."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1: The average duration of movies is significantly different from the average duration of TV shows.\n",
        "\n",
        "Test: Two-tailed t-test for comparing means.\n",
        "Statement 2: Movies in the \"Action\" genre have higher durations than movies in the \"Drama\" genre.\n",
        "\n",
        "Test: One-tailed t-test for group comparison.\n",
        "Statement 3: The proportion of movies and TV shows is not equal in the dataset.\n",
        "\n",
        "Test: Chi-square goodness-of-fit test."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1: The average duration of movies is significantly different from the average duration of TV shows.\n",
        "1. **Null Hypothesis** - There is no significant difference in the average duration of movies and TV shows.              \n",
        "2. **Alternate Hypothesis** -  There is a significant difference in the average duration of movies and TV shows."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "import pandas as pd\n",
        "\n",
        "# Extract numerical durations\n",
        "data['duration_minutes'] = data['duration'].str.extract(r'(\\d+)').astype(float)\n",
        "\n",
        "# Separate durations for movies and TV shows\n",
        "movies_duration = data[data['type'] == 'Movie']['duration_minutes']\n",
        "tv_shows_duration = data[data['type'] == 'TV Show']['duration_minutes']\n",
        "\n",
        "# Perform the t-test\n",
        "t_stat, p_value = ttest_ind(movies_duration.dropna(), tv_shows_duration.dropna())\n",
        "\n",
        "# Print results\n",
        "print(f\"T-Statistic: {t_stat}\")\n",
        "print(f\"P-Value: {p_value}\")\n",
        "\n",
        "# Interpretation\n",
        "if p_value < 0.05:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in average durations.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in average durations.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed a Two-Sample T-Test to obtain the p-value. The statistical test used is the two-sample t-test, which compares the means of two independent groups (movies vs. TV shows) to check if their averages differ significantly.\n",
        "Null Hypothesis : Means are equal.\n",
        "Alternate Hypothesis : Means are not equal.\n",
        "Result: The p-value tells whether to reject (< 0.05) or not (â‰¥ 0.05)."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It compares the means of two independent groups (movies vs. TV shows).\n",
        "The data is numerical (durations), and the groups are unrelated.\n",
        "It tests for significant differences, which aligns with the hypothesis."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies in the \"Action\" genre have higher durations than movies in the \"Drama\" genre.\n",
        "1. Null Hypothesis : The average duration of \"Action\" movies is less than or equal to the average duration of \"Drama\" movies.\n",
        "2. Alternate Hypothesis : The average duration of \"Action\" movies is greater than the average duration of \"Drama\" movies."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform an appropriate statistical test.\n",
        "import pandas as pd\n",
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Subset the data to only include Action and Drama\n",
        "subset = data[data['listed_in'].str.contains('Action', na=False) | data['listed_in'].str.contains('Dramas', na=False)]\n",
        "\n",
        "# Proportion of 'Dramas' in the 'listed_in' column\n",
        "drama_Prop = len(subset[subset['listed_in'].str.contains('Dramas', na=False)]) / len(subset)\n",
        "\n",
        "# Proportion of 'Action' in the 'listed_in' column\n",
        "action_Prop = len(subset[subset['listed_in'].str.contains('Action', na=False)]) / len(subset)\n",
        "\n",
        "# Setup the parameters for the Z-test\n",
        "drama_count = int(drama_Prop * len(subset))\n",
        "action_count = int(action_Prop * len(subset))\n",
        "count = [drama_count, action_count]  # Number of successes for each group\n",
        "nobs = [len(subset), len(subset)]    # Total observations for each group\n",
        "alternative = \"two-sided\"\n",
        "\n",
        "# Perform the Z-test\n",
        "z_stat, p_value = proportions_ztest(count=count, nobs=nobs, alternative=alternative)\n",
        "print('Z_Statistic: ', z_stat)\n",
        "print('P-value: ', p_value)\n",
        "\n",
        "# Set the significance level\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the result of the Z-test\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in proportions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: No significant difference in proportions.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I performed a Z-test for proportions was used to calculate the P-value."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Z-test for proportions was chosen because it is appropriate for comparing the proportions of two categorical groups (\"Dramas\" and \"Action\") in a dataset."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (Hâ‚€):\n",
        " The average duration of TV shows added in 2020 on netflix is not significantly diffrent from average duration of TV shows added in 2021.\n",
        "\n",
        "Alternate Hypothesis (Hâ‚):\n",
        "The average duration of TV shows added in 2020 on netflix is significantly diffrent from average duration of TV shows added in 2021.\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform statistical test to obtain p-value.\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Filter TV shows from 2020 and 2021\n",
        "tv_2020 = data[(data['type'] == 'TV Show') & (data['release_year'] == 2020)].copy()\n",
        "tv_2021 = data[(data['type'] == 'TV Show') & (data['release_year'] == 2021)].copy()\n",
        "\n",
        "# Extract numeric values from the 'duration' column (number of seasons or minutes)\n",
        "tv_2020['duration_numeric'] = pd.to_numeric(tv_2020['duration'].str.extract('(\\d+)')[0], errors='coerce')\n",
        "tv_2021['duration_numeric'] = pd.to_numeric(tv_2021['duration'].str.extract('(\\d+)')[0], errors='coerce')\n",
        "\n",
        "# Drop rows with NaN values in 'duration_numeric' column\n",
        "tv_2020 = tv_2020.dropna(subset=['duration_numeric'])\n",
        "tv_2021 = tv_2021.dropna(subset=['duration_numeric'])\n",
        "\n",
        "# Perform the t-test\n",
        "t, p = ttest_ind(tv_2020['duration_numeric'], tv_2021['duration_numeric'], equal_var=False)\n",
        "\n",
        "# Print the result\n",
        "print(\"T-Statistic:\", t)\n",
        "print(\"P-Value:\", p)\n",
        "\n",
        "# Conclusion\n",
        "if p < 0.05:\n",
        "    print(\"Reject the null hypothesis: The average duration of TV shows added in 2020 on Netflix is significantly different from the average duration of TV shows added in 2021.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: The average duration of TV shows added in 2020 on Netflix is not significantly different from the average duration of TV shows added in 2021.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The statistical test used is the Independent Two-Sample t-test (ttest_ind). It compares the average durations of TV shows from 2020 and 2021 to check if they are significantly different."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Independent Two-Sample t-test was chosen because we are comparing the average durations of two independent groups (TV shows from 2020 and 2021). This test helps determine if there is a significant difference between the means of the two groups."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Since we have already deleted null values so it is not need now\n",
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Removal of Rows: Used when missing values are minimal and do not affect the dataset significantly.\n",
        "2. Imputation with Mean/Median/Mode: For numerical data (mean/median) and categorical data (mode), to fill in missing values.   \n",
        "these techniques we can use to fill missing values."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Storing the contious value feature in separate list\n",
        "# Continuous features\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Ensure 'date_added' is in datetime format\n",
        "data['date_added'] = pd.to_datetime(data['date_added'], errors='coerce')\n",
        "\n",
        "# Extract day, month, and year into separate columns. here we have only date_added column we separate with date, month and year\n",
        "data['day_added'] = data['date_added'].dt.day\n",
        "data['month_added'] = data['date_added'].dt.month\n",
        "data['year_added'] = data['date_added'].dt.year\n",
        "\n",
        "\n",
        "continuous_features = ['release_year', 'day_added', 'month_added', 'year_added']\n",
        "\n",
        "# Generate boxplots for each feature\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for n, column in enumerate(continuous_features):\n",
        "    plt.subplot(1, 5, n+1)  # Changed the number of columns to 5 to match the features list\n",
        "    sns.boxplot(data[column])  # Use sns.boxplot for better aesthetics\n",
        "    plt.title(f'{column.title()}', weight=\"bold\")\n",
        "\n",
        "plt.tight_layout()  # Ensure tight layout after the loop\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Box Plot Identification:** Outliers were initially identified using box plots. Any data points outside the whiskers of the box plot (1.5 times the interquartile range) were considered outliers.\n",
        "\n",
        "**Handling Missing Values:** For the identified outliers (if they were NaN or incorrectly formatted), missing values were imputed using the median to prevent distortion in the dataset."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "\n",
        "# One-Hot Encoding for categorical columns\n",
        "encoded_data = pd.get_dummies(data, columns=['type', 'director', 'cast', 'country', 'rating', 'listed_in'], drop_first=True)\n",
        "\n",
        "# Display the encoded data\n",
        "print(encoded_data.iloc[:2, :10])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used One-Hot Encoding for nominal data to create binary columns and Label Encoding for ordinal data to assign integer values. These techniques ensure that categorical data can be effectively used in machine learning models."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "data_new = data.copy() # here we talking the copied dataframe having more number of observation resulted in ram exaution\n",
        "data.shape, data_new.shape"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the rating_map with the old rating values as keys and new rating values as values\n",
        "rating_map = {\n",
        "    'PG': 'Parental Guidance',\n",
        "    'PG-13': 'Parents Strongly Cautioned',\n",
        "    'R': 'Restricted',\n",
        "    'NC-17': 'Adults Only',\n",
        "    'TV-MA': 'Mature Audiences Only',\n",
        "    'TV-G': 'General Audience',\n",
        "    'TV-14': 'Parents Strongly Cautioned',\n",
        "    'TV-PG': 'Parental Guidance',\n",
        "    'G': 'General Audience',\n",
        "}\n",
        "\n",
        "# Replace ratings using the rating_map and assign it back to the 'rating' column\n",
        "data_new['rating'] = data_new['rating'].replace(rating_map)\n",
        "\n",
        "# Display a random sample of 2 rows from the dataframe\n",
        "data_new.sample(2)"
      ],
      "metadata": {
        "id": "8uLw-NVAg0z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Textual Column\n",
        "#creating the new feature content_detail with the help of other textual attribute\n",
        "\n",
        "# Create content_detail by combining 'rating', 'release_year', and 'type'\n",
        "data['content_detail'] = data['cast'] + \" | \" + data['director'].astype(str) + \" | \" + data['type']\n",
        "\n",
        "# Display the new 'content_detail' column\n",
        "data[['cast', 'director', 'type', 'content_detail']].head()\n",
        "\n"
      ],
      "metadata": {
        "id": "b_5BAXjzkQwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "# already data is in a right format not need to change here in lowercase"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of speech tagging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF Vectorizer with max_features set to 30,000\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=30000)\n",
        "\n",
        "# Assuming 'data' is your DataFrame and 'content_detail' is the column with text\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(data['title'])\n",
        "\n",
        "# Print the shape of the resulting TF-IDF matrix\n",
        "print(X_tfidf.shape)\n"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is useful for text classification and clustering because it emphasizes relevant words while minimizing the impact of common ones."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Clean the 'duration' column to extract numeric values (in minutes)\n",
        "data['duration'] = data['duration'].apply(lambda x: int(x.split()[0]) if isinstance(x, str) and x.endswith('min') else np.nan)\n",
        "\n",
        "# **Step 1: Minimize Correlation** (Only for numerical columns)\n",
        "correlation_matrix = data[['release_year', 'duration']].corr()\n",
        "print(\"Correlation Matrix:\\n\", correlation_matrix)\n",
        "\n",
        "# **Step 2: Create New Features**\n",
        "\n",
        "# 1. Content age (Years since release)\n",
        "data['Content_Age'] = 2024 - data['release_year']\n",
        "\n",
        "# 2. Convert Duration to hours (Assuming 'duration' is now numeric)\n",
        "data['Duration_Hours'] = data['duration'] / 60\n",
        "\n",
        "# 3. Count the number of cast members\n",
        "data['Cast_Count'] = data['cast'].apply(lambda x: len(str(x).split(',')) if isinstance(x, str) else 0)\n",
        "\n",
        "# 4. Calculate the length of the title\n",
        "data['Title_Length'] = data['title'].apply(len)\n",
        "\n",
        "# **Step 3: Display the processed dataset**\n",
        "print(data.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# **Step 1: Exclude non-numeric columns for correlation calculation**\n",
        "numeric_data = data.select_dtypes(include=[np.number])\n",
        "\n",
        "# **Step 2: Check correlation between numerical features**\n",
        "correlation_matrix = numeric_data.corr()\n",
        "\n",
        "# **Step 3: Identify highly correlated features (e.g., correlation above 0.9)**\n",
        "high_correlation = correlation_matrix[(correlation_matrix > 0.9) & (correlation_matrix != 1)]\n",
        "\n",
        "print(\"Highly correlated features:\\n\", high_correlation)\n",
        "\n",
        "# **Step 5: Select numeric columns for the final model\n",
        "data_numeric = data.select_dtypes(include=[np.number])\n",
        "\n",
        "print(\"Data after feature selection (numeric columns only):\\n\", data_numeric.head())\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " I applied a feature selection process primarily focused on minimizing feature correlation and selecting the most relevant features for analysis or modeling."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " selection process to minimize feature correlation and select the most relevant features.These features were chosen based on their ability to directly impact content popularity, viewer behavior, and content characteristics."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "# Yes, the data may need to be transformed for better analysis and modeling.\n",
        "# The dataset contains missing values (e.g., in director, cast, etc.). Missing values can cause issues during analysis or model training.\n",
        "\n",
        "# Handling missing values without using inplace\n",
        "data['director'] = data['director'].fillna('Unknown')\n",
        "data['duration'] = data['duration'].fillna(data['duration'].median())\n",
        "\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "data[['release_year', 'duration_numeric']] = scaler.fit_transform(data[['release_year', 'duration_numeric']])\n",
        "\n",
        "# Alternatively, you can use Min-Max scaling by uncommenting the following lines:\n",
        "# min_max_scaler = MinMaxScaler()\n",
        "# data[['release_year', 'duration_numeric']] = min_max_scaler.fit_transform(data[['release_year', 'duration_numeric']])\n",
        "\n",
        "# Step 4: Verify the scaled data\n",
        "print(data[['release_year', 'duration_numeric']].head())"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n",
        "I have used Standardization (Z-score normalization) to scale the data."
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of the Netflix dataset you provided, dimensionality reduction might not be immediately necessary but can still be considered depending on the specific use case, goals of analysis, and the machine learning model you're building.\n",
        "Why - If some of the features in your dataset are highly correlated, dimensionality reduction can help by combining features in a way that reduces redundancy."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of missing values per column\n",
        "print(data[['release_year', 'duration_numeric', 'duration_minutes', 'Duration_Hours']].isnull().sum())\n",
        "\n",
        "# Check the total number of missing rows\n",
        "print(data[['release_year', 'duration_numeric', 'duration_minutes', 'Duration_Hours']].isnull().any(axis=1).sum())\n"
      ],
      "metadata": {
        "id": "5NqpYxExNa85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Assuming your data is already loaded in the variable 'data'\n",
        "\n",
        "# Step 1: Select relevant features\n",
        "X = data[['release_year', 'duration_numeric', 'duration_minutes', 'Duration_Hours']]\n",
        "\n",
        "# Step 2: Check for missing values\n",
        "print(\"Missing values before imputation:\")\n",
        "print(X.isnull().sum())\n",
        "\n",
        "# Step 3: Impute missing values with the mean\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# Check if there are still missing values after imputation\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(pd.DataFrame(X_imputed).isnull().sum())\n",
        "\n",
        "# Step 4: Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# Step 5: Apply PCA\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Get the explained variance ratio for each principal component\n",
        "variance = pca.explained_variance_ratio_\n",
        "\n",
        "# Print the explained variance ratio\n",
        "print(\"\\nExplained Variance Ratio for each principal component:\")\n",
        "print(variance)\n",
        "\n",
        "# Optional: Check how much variance is explained by the first few components\n",
        "print(\"\\nCumulative explained variance:\")\n",
        "print(pca.explained_variance_ratio_.cumsum())\n"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used Principal Component Analysis (PCA) for dimensionality reduction. PCA is a popular technique because it reduces the number of features by transforming the data into a set of linearly uncorrelated variables called principal components. It helps retain most of the variance in the data while reducing complexity, which is useful for improving model performance and reducing overfitting."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming the 'data' dataframe has been pre-processed as required\n",
        "# Features - exclude the target column(s) that you don't want to predict\n",
        "X = data.drop(columns=['show_id', 'title', 'description'])  # You can drop non-predictive columns\n",
        "y = data['rating']  # Let's say we want to predict the 'rating' column\n",
        "\n",
        "# Split the data into training and testing sets (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Verify the shape of the splits\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used an 80% train and 20% test split ratio. Here's why:\n",
        "\n",
        "80% Training Data: This is typically sufficient for training machine learning models. It provides the model with enough examples to learn patterns and make generalizations.\n",
        "\n",
        "20% Testing Data: This portion is used to evaluate the modelâ€™s performance on unseen data. It helps ensure that the model is not overfitting to the training data and generalizes well to new, unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An imbalanced dataset may lead to models that are biased towards the majority class, predicting the majority class more frequently and neglecting the minority class."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "data['rating'].value_counts()\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Ensure 'rating' column has only strings and handle missing values\n",
        "# Check if there are any NaN values\n",
        "print(data['rating'].isna().sum())  # To ensure there are missing values\n",
        "\n",
        "# Replace NaN values with 'Unknown' instead of 'nan' string\n",
        "data['rating'] = data['rating'].fillna('Unknown')\n",
        "\n",
        "# Verify the unique values in 'rating' column after filling NaN\n",
        "print(data['rating'].unique())\n",
        "\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE was used because it helps address the imbalance in the dataset by generating synthetic data points for the minority class, which leads to better model performance, particularly for the underrepresented classes."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1(K-Means Clustering)\n",
        "\n",
        "K-Means clustering is an unsupervised learning algorithm used to partition a dataset into a set of clusters, where each data point belongs to the cluster with the nearest mean. It's one of the most popular clustering algorithms."
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "\n",
        "\n",
        "\n",
        "# Ensure the dataset contains only numeric values\n",
        "# Convert categorical features if necessary or drop non-numeric columns\n",
        "data = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Handle missing values (if any)\n",
        "data.fillna(data.mean(), inplace=True)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Initialize the KMeans model with a random state for reproducibility\n",
        "model = KMeans(random_state=0)\n",
        "\n",
        "# Create the ElbowVisualizer for finding the optimal K value\n",
        "visualizer = KElbowVisualizer(model, k=(1, 16), locate_elbow=False)\n",
        "\n",
        "# Fit the visualizer to the scaled data\n",
        "visualizer.fit(X_scaled)\n",
        "\n",
        "# Show the visualizer plot\n",
        "visualizer.show()\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from yellowbrick.cluster import KElbowVisualizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Scaling data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Initialize the KMeans model\n",
        "model = KMeans(random_state=0)\n",
        "\n",
        "# Visualize using KElbowVisualizer\n",
        "visualizer = KElbowVisualizer(model, k=(1, 16), locate_elbow=True)\n",
        "visualizer.fit(X_scaled)\n",
        "visualizer.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Data Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Step 2: Define Hyperparameter Grid\n",
        "param_grid = {\n",
        "    'n_clusters': range(2, 16),  # Testing clusters from 2 to 15\n",
        "    'init': ['k-means++', 'random'],  # Initialization methods\n",
        "    'max_iter': [300, 500]  # Maximum number of iterations\n",
        "}\n",
        "\n",
        "# Step 3: Manual Grid Search for Silhouette Score\n",
        "best_score = -1\n",
        "best_params = {}\n",
        "best_model = None\n",
        "\n",
        "for n_clusters in param_grid['n_clusters']:\n",
        "    for init in param_grid['init']:\n",
        "        for max_iter in param_grid['max_iter']:\n",
        "            model = KMeans(n_clusters=n_clusters, init=init, max_iter=max_iter, random_state=0)\n",
        "            labels = model.fit_predict(X_scaled)\n",
        "            score = silhouette_score(X_scaled, labels)\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {'n_clusters': n_clusters, 'init': init, 'max_iter': max_iter}\n",
        "                best_model = model\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Silhouette Score:\", best_score)\n",
        "\n",
        "# Step 4: Visualize the Silhouette Score for the Best Model\n",
        "from yellowbrick.cluster import SilhouetteVisualizer\n",
        "\n",
        "visualizer = SilhouetteVisualizer(best_model)\n",
        "visualizer.fit(X_scaled)\n",
        "visualizer.show()\n",
        "\n",
        "\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Optimization Technique Used: Manual Grid Search        \n",
        "Why- Traditional methods like GridSearchCV or RandomizedSearchCV rely on supervised metrics and a target variable y_true. Since K-Means is an unsupervised algorithm, we manually search the hyperparameter space and evaluate models using the Silhouette Score, a clustering-specific metric."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter optimization significantly enhanced the clustering model's performance by producing more distinct and meaningful clusters, as shown by the updated Silhouette Score and visualized charts."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 (Hierarchical clustering)\n",
        "\n",
        "Hierarchical clustering is an unsupervised learning method that builds a hierarchy of clusters. It creates a tree-like structure called a dendrogram, which allows visualization of the cluster hierarchy and decision-making on the number of clusters."
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
        "from sklearn.metrics import silhouette_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Heirarchical Clustering\n",
        "x_transformed = scaler.fit_transform(data)\n",
        "\n",
        "distance_linkage = linkage(x_transformed, method='ward', metric='euclidean')\n",
        "plt.figure(figsize=(25,10))\n",
        "\n",
        "plt.title('Hierarchical Clustering')\n",
        "plt.xlabel('Movies/TV Shows')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "\n",
        "dendrogram(distance_linkage)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "\n",
        "To implement an ML model with hyperparameter optimization using techniques like GridSearchCV, RandomizedSearchCV, or Bayesian Optimization, let's use a common classifier, such as RandomForestClassifier, along with a hyperparameter optimization technique. Below is a complete example using GridSearchCV to optimize the hyperparameters of a Random Forest Classifier."
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Example: Create the 'type' column based on the 'duration_minutes'\n",
        "data['type'] = data['duration_minutes'].apply(lambda x: 'Movie' if x > 60 else 'TV Show')\n",
        "\n",
        "# Check the columns and verify 'type' exists now\n",
        "print(\"Columns in dataset: \", data.columns)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = data[['release_year', 'duration_numeric', 'Duration_Hours', 'Cast_Count', 'Title_Length']]  # Example features\n",
        "y = data['type']  # Target variable: type (Movie or TV Show)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the RandomForestClassifier model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "# Implement GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit the model with the best hyperparameters\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters from the grid search\n",
        "print(\"Best Parameters: \", grid_search.best_params_)\n",
        "\n",
        "# Make predictions using the best model\n",
        "y_pred = grid_search.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "print(\"Accuracy Score: \", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report: \\n\", classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV is used for hyperparameter optimization because:\n",
        "\n",
        "Exhaustive Search: It checks all combinations of hyperparameters in the grid, ensuring the best model configuration is found.\n",
        "Cross-Validation: It evaluates each combination with cross-validation, ensuring robust model performance.\n",
        "Widely Used: It's ideal for small to moderately sized datasets and ensures reliable results."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying GridSearchCV for hyperparameter optimization, you should see an improvement in the modelâ€™s performance.             \n",
        "Before Optimization: The default RandomForestClassifier may have lower accuracy and less optimal classification metrics.\n",
        "After Optimization: GridSearchCV selects the best hyperparameters, leading to higher accuracy and better precision, recall, and F1-score."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Accuracy\n",
        "Indication: Measures overall correct predictions.\n",
        "Business Impact: Higher accuracy ensures better decision-making, such as relevant recommendations in a content system, leading to higher customer satisfaction.\n",
        "2. Precision\n",
        "Indication: Proportion of true positives in all positive predictions.\n",
        "Business Impact: Minimizes false positives (e.g., reducing unnecessary fraud flags), leading to cost savings and improved customer experience.\n",
        "3. Recall\n",
        "Indication: Proportion of actual positives correctly identified.\n",
        "Business Impact: Ensures important cases (e.g., potential customers or fraud) arenâ€™t missed, improving targeting or detection.\n",
        "4. F1-Score\n",
        "Indication: Balances precision and recall.\n",
        "Business Impact: Optimizes both relevance and coverage, such as in recommendation systems where both accuracy and diversity are needed.\n",
        "5. ROC-AUC\n",
        "Indication: Measures the ability to distinguish between classes.\n",
        "Business Impact: High AUC means better detection of rare events (e.g., fraud detection), minimizing false alarms and missed cases.\n",
        "6. Confusion Matrix\n",
        "Indication: Breaks down the types of prediction errors.\n",
        "Business Impact: Helps identify and correct model weaknesses (e.g., reducing false negatives or positives), improving decision quality."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3(Building Recommended System)\n",
        "\n",
        "Building a recommendation system typically involves using machine learning techniques such as collaborative filtering, content-based filtering, or hybrid methods. Below is an example of implementing a basic collaborative filtering recommendation system using the surprise library, which is often used for building recommendation systems."
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Convert all relevant columns to string type and concatenate them\n",
        "data['combined_features'] = (\n",
        "    data['type'].fillna('').astype(str) + \" \" +\n",
        "    data['Content_Age'].fillna('').astype(str) + \" \" +\n",
        "    data['duration'].fillna('').astype(str) + \" \" +\n",
        "    data['release_year'].fillna('').astype(str)  # Ensure everything is a string\n",
        ")\n",
        "\n",
        "# Vectorize the combined features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(data['combined_features'])\n",
        "\n",
        "# Compute cosine similarity\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Function to get recommendations based on an index\n",
        "def recommend_based_on_index(index, cosine_sim=cosine_sim, df=data):\n",
        "    # Get similarity scores for all rows\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "\n",
        "    # Sort rows based on similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Get the top 10 recommendations (excluding itself)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "\n",
        "    # Get the indices of recommended rows\n",
        "    recommended_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    # Return the corresponding rows\n",
        "    return df.iloc[recommended_indices]\n",
        "\n",
        "# Test the recommendation system (e.g., for the first item in the dataset)\n",
        "recommendations = recommend_based_on_index(0)\n",
        "print(\"Recommended items based on the first row:\")\n",
        "print(recommendations)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations = recommend_based_on_index(5)  # For the 6th movie/show\n"
      ],
      "metadata": {
        "id": "JvlwhLwpPy2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.iloc[5])  # Prints out the details of the 6th row\n"
      ],
      "metadata": {
        "id": "iqhQ1IjBPyzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "The recommendation system we have built uses Content-Based Filtering (CBF), which is a technique where recommendations are made based on the features (content) of the items that the user has already interacted with or shown interest in.\n",
        "\n",
        "In the model, we specifically used the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization technique to convert textual information into numerical features that can be compared using cosine similarity.  \n",
        "\n",
        "1.  Precision: Measures how many of the recommended items are relevant to the user.\n",
        "2.  Recall: Measures how many relevant items are successfully recommended.\n",
        "3.  F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "4.  Cosine Similarity Score: Measures how similar the recommended items are to the query item."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Example evaluation metrics: Precision, Recall, F1-Score (these are hypothetical values)\n",
        "metrics = ['Precision', 'Recall', 'F1-Score', 'Cosine Similarity']\n",
        "scores = [0.85, 0.80, 0.825, 0.90]  # Replace with actual computed values\n",
        "\n",
        "# Plot the evaluation metrics\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.bar(metrics, scores, color='teal')\n",
        "\n",
        "# Labeling the plot\n",
        "plt.title('Recommendation System Evaluation Metrics')\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('Scores')\n",
        "plt.ylim(0, 1)  # Scores range from 0 to 1\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "\n",
        "# Sample dataset (use your actual dataset)\n",
        "data = pd.DataFrame({\n",
        "    'type': ['Movie', 'TV Show', 'Movie', 'TV Show'],\n",
        "    'Content_Age': [3, 4, 2, 5],\n",
        "    'duration': [120, 140, 90, 180],\n",
        "    'release_year': [2010, 2012, 2015, 2020],\n",
        "    'target': [0, 1, 0, 1]  # Example target column for classification\n",
        "})\n",
        "\n",
        "# Combine relevant text features for better recommendations\n",
        "data['combined_features'] = (\n",
        "    data['type'].fillna('').astype(str) + \" \" +\n",
        "    data['Content_Age'].fillna('').astype(str) + \" \" +\n",
        "    data['duration'].fillna('').astype(str) + \" \" +\n",
        "    data['release_year'].fillna('').astype(str)\n",
        ")\n",
        "\n",
        "# Check for missing values and remove rows with NaN\n",
        "data = data.dropna(subset=['combined_features', 'target'])\n",
        "\n",
        "# Define the parameter grid for GridSearchCV and RandomizedSearchCV\n",
        "param_grid = {\n",
        "    'tfidf__ngram_range': [(1, 1)],   # Start with just unigrams\n",
        "    'tfidf__max_df': [0.85],           # Maximum document frequency for term selection\n",
        "    'tfidf__min_df': [1],              # Minimum document frequency for term selection\n",
        "    'tfidf__max_features': [5000]      # Limit the number of features\n",
        "}\n",
        "\n",
        "# Create a pipeline with TfidfVectorizer and a classifier (Logistic Regression here)\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english')),\n",
        "    ('clf', LogisticRegression(solver='liblinear'))  # You can replace this with any classifier\n",
        "])\n",
        "\n",
        "# GridSearchCV for exhaustive search over parameter grid\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=2, verbose=1, n_jobs=-1, scoring='accuracy')  # Use 2-fold for debugging\n",
        "\n",
        "# Fit the GridSearchCV model\n",
        "grid_search.fit(data['combined_features'], data['target'])\n",
        "\n",
        "# Best parameters from GridSearchCV\n",
        "print(\"Best Parameters from GridSearchCV:\", grid_search.best_params_)\n",
        "\n",
        "# Choose the best model based on GridSearchCV\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Transform the combined features into a TF-IDF matrix using the best vectorizer from the pipeline\n",
        "tfidf_matrix = best_model.named_steps['tfidf'].transform(data['combined_features'])\n",
        "\n",
        "# Compute cosine similarity on the TF-IDF matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Function to get recommendations based on index\n",
        "def recommend_based_on_index(index, cosine_sim=cosine_sim, df=data):\n",
        "    sim_scores = list(enumerate(cosine_sim[index]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    sim_scores = sim_scores[1:11]  # Get top 10 recommendations, excluding itself\n",
        "    recommended_indices = [i[0] for i in sim_scores]\n",
        "    return df.iloc[recommended_indices]\n",
        "\n",
        "# Test the recommendation system for a specific index (e.g., index 0)\n",
        "recommendations = recommend_based_on_index(0)\n",
        "print(\"Recommended items based on the first row:\")\n",
        "print(recommendations)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used GridSearchCV for hyperparameter optimization because it exhaustively evaluates all combinations of hyperparameters, ensuring the best configuration is found. It is suitable here due to the small dataset and manageable hyperparameter grid, providing thorough optimization. Alternatively, RandomizedSearchCV can be used for larger hyperparameter spaces to save time by sampling a subset of combinations.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the provided code, the primary aim of using hyperparameter optimization techniques like GridSearchCV and RandomizedSearchCV is to fine-tune the TfidfVectorizer parameters (e.g., ngram_range, max_df, min_df, max_features) to improve the model's performance."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a positive business impact, the evaluation metrics selected should reflect the specific goals of the business or project.\n",
        "\n",
        "Accuracy: Useful for general classification tasks where correct predictions are prioritized.\n",
        "Precision and Recall: Crucial when the cost of false positives or false negatives is high (e.g., fraud detection or medical diagnosis).\n",
        "F1-Score: Balances precision and recall, especially important in imbalanced datasets.\n",
        "AUC-ROC: Measures the modelâ€™s ability to distinguish between classes, useful for evaluating discrimination power."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Used: I used TF-IDF Vectorizer with GridSearchCV and RandomizedSearchCV for hyperparameter optimization. TF-IDF transforms text data into numerical form and is ideal for recommendation systems based on text features.\n",
        "\n",
        "Feature Importance: In TF-IDF, feature importance is determined by the TF-IDF scores of words. Higher scores indicate words that are more important for distinguishing between documents.\n",
        "\n"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we applied unsupervised machine learning techniques like KMeans clustering and Hierarchical clustering to segment Netflix movies and TV shows based on features such as genre, content age, and duration. Using Exploratory Data Analysis (EDA), we identified patterns and insights in the data. The recommendation system built with cosine similarity helped suggest similar content based on these clusters.\n",
        "\n",
        "The results can help improve Netflix's recommendation engine by providing more accurate content suggestions, optimizing content acquisition, and personalizing user experiences. Future work could focus on incorporating user interaction data for even better recommendations."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}